{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acd92929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset,Subset\n",
    "from torchvision import transforms,datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "import os\n",
    "from torch.utils.data import random_split\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4c6a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super(PrototypicalNetwork, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        # Start with encoder unfrozen by default\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    def forward(self, support, query):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            support: [n_way, k_shot, C, H, W]\n",
    "            query: [n_way * n_query, C, H, W]\n",
    "        Returns:\n",
    "            logits: [n_way * n_query, n_way]\n",
    "        \"\"\"\n",
    "        n_way, k_shot = support.shape[:2]\n",
    "        \n",
    "        # Flatten support and encode\n",
    "        support_flat = support.view(-1, *support.shape[-3:])\n",
    "        support_features = self.encoder(support_flat)  # [n_way*k_shot, feature_dim]\n",
    "        \n",
    "        # Encode query images\n",
    "        query_features = self.encoder(query)  # [n_way*n_query, feature_dim]\n",
    "        \n",
    "        # Reshape support features and compute prototypes\n",
    "        support_features = support_features.view(n_way, k_shot, -1)  # [n_way, k_shot, feature_dim]\n",
    "        prototypes = torch.mean(support_features, dim=1)  # [n_way, feature_dim]\n",
    "        \n",
    "        # Compute squared Euclidean distances\n",
    "        distances = torch.cdist(query_features, prototypes, p=2).pow(2)\n",
    "        \n",
    "        return -distances  # Negative distances as logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d39d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim=64, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, hidden_dim, kernel_size=3, padding=1),  # 28x28\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> 14x14\n",
    "\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> 7x7\n",
    "\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # -> 3x3\n",
    "\n",
    "            nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # -> 1x1\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)          # Shape: (B, hidden_dim, 1, 1)\n",
    "        x = x.view(x.size(0), -1)    # Flatten to (B, hidden_dim)\n",
    "        return self.fc(x)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bea99fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Temp\\ipykernel_15216\\1573264925.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"simsiam_encoder.pth\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.load_state_dict(torch.load(\"simsiam_encoder.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c23a7169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class TrainingVisualizer:\n",
    "    def __init__(self):\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_accs = []\n",
    "        self.val_accs = []\n",
    "        plt.ion()  # Interactive mode on\n",
    "        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    def update(self, train_loss, val_loss, train_acc, val_acc):\n",
    "        \"\"\"Update the plots with new metrics\"\"\"\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.train_accs.append(train_acc)\n",
    "        self.val_accs.append(val_acc)\n",
    "        \n",
    "        # Clear previous plots\n",
    "        self.ax1.clear()\n",
    "        self.ax2.clear()\n",
    "        \n",
    "        # Plot loss\n",
    "        self.ax1.plot(self.train_losses, label='Train Loss', color='blue')\n",
    "        self.ax1.plot(self.val_losses, label='Val Loss', color='orange')\n",
    "        self.ax1.set_title('Training & Validation Loss')\n",
    "        self.ax1.set_xlabel('Epoch')\n",
    "        self.ax1.set_ylabel('Loss')\n",
    "        self.ax1.legend()\n",
    "        self.ax1.grid(True)\n",
    "        \n",
    "        # Plot accuracy\n",
    "        self.ax2.plot(self.train_accs, label='Train Accuracy', color='green')\n",
    "        self.ax2.plot(self.val_accs, label='Val Accuracy', color='red')\n",
    "        self.ax2.set_title('Training & Validation Accuracy')\n",
    "        self.ax2.set_xlabel('Epoch')\n",
    "        self.ax2.set_ylabel('Accuracy (%)')\n",
    "        self.ax2.legend()\n",
    "        self.ax2.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.pause(0.1)  # Pause to update the display\n",
    "    \n",
    "    def save(self, save_dir='results'):\n",
    "        \"\"\"Save the final plots to disk\"\"\"\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # Save as PNG\n",
    "        plt.savefig(os.path.join(save_dir, 'training_curves.png'))\n",
    "        \n",
    "        # Save data for later analysis\n",
    "        torch.save({\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'train_accs': self.train_accs,\n",
    "            'val_accs': self.val_accs\n",
    "        }, os.path.join(save_dir, 'training_history.pt'))\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "af411415",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Fixed Dataset Class\n",
    "class ScriptDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, augment=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = self._make_dataset()\n",
    "        \n",
    "    def _make_dataset(self):\n",
    "        samples = []\n",
    "        for class_name in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                samples.append((img_path, self.class_to_idx[class_name]))\n",
    "        return samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        img = Image.open(img_path).convert('L')  # Grayscale\n",
    "        img = transforms.Resize((28, 28))(img)\n",
    "        \n",
    "        if self.augment:\n",
    "            img = transforms.RandomAffine(\n",
    "                degrees=15, translate=(0.1, 0.1), scale=(0.8, 1.2)\n",
    "            )(img)\n",
    "            img = transforms.RandomPerspective(distortion_scale=0.2, p=0.5)(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        return img, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "# 2. Fixed Data Splitting\n",
    "def get_datasets(root_dir, val_ratio=0.2):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    full_dataset = ScriptDataset(root_dir, transform=transform, augment=True)\n",
    "    \n",
    "    # Create indices for splitting\n",
    "    num_samples = len(full_dataset)\n",
    "    indices = list(range(num_samples))\n",
    "    split = int(np.floor(val_ratio * num_samples))\n",
    "    \n",
    "    # Shuffle indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Split into train and val\n",
    "    train_idx, val_idx = indices[split:], indices[:split]\n",
    "    train_dataset = Subset(full_dataset, train_idx)\n",
    "    val_dataset = Subset(full_dataset, val_idx)\n",
    "    \n",
    "    # Disable augmentation for validation\n",
    "    full_dataset.augment = False  # Affects validation through Subset\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# 3. Fixed Episode Iterator\n",
    "class EpisodeIterator:\n",
    "    def __init__(self, subset, n_way=5, k_shot=5, n_query=15):\n",
    "        self.subset = subset\n",
    "        self.n_way = n_way\n",
    "        self.k_shot = k_shot\n",
    "        self.n_query = n_query\n",
    "        \n",
    "        # Get the original dataset from Subset\n",
    "        self.dataset = subset.dataset\n",
    "        self.indices = subset.indices\n",
    "        \n",
    "        # Create label to indices mapping\n",
    "        self.label_to_indices = {}\n",
    "        for idx in self.indices:\n",
    "            _, label = self.dataset.samples[idx]\n",
    "            if label not in self.label_to_indices:\n",
    "                self.label_to_indices[label] = []\n",
    "            self.label_to_indices[label].append(idx)\n",
    "        \n",
    "        self.available_classes = list(self.label_to_indices.keys())\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        # Select n_way random classes with enough samples\n",
    "        selected_classes = random.sample(\n",
    "            [cls for cls in self.available_classes \n",
    "             if len(self.label_to_indices[cls]) >= self.k_shot + self.n_query],\n",
    "            self.n_way\n",
    "        )\n",
    "        \n",
    "        support = []\n",
    "        query = []\n",
    "        \n",
    "        for class_idx in selected_classes:\n",
    "            # Get all samples for this class\n",
    "            class_indices = self.label_to_indices[class_idx]\n",
    "            selected = random.sample(class_indices, self.k_shot + self.n_query)\n",
    "            \n",
    "            # Add to support and query sets\n",
    "            support.extend(selected[:self.k_shot])\n",
    "            query.extend(selected[self.k_shot:])\n",
    "        \n",
    "        # Convert indices to actual data\n",
    "        support_data = [self.dataset[i] for i in support]\n",
    "        query_data = [self.dataset[i] for i in query]\n",
    "        \n",
    "        # Stack into tensors\n",
    "        support_imgs = torch.stack([img for img, _ in support_data])\n",
    "        query_imgs = torch.stack([img for img, _ in query_data])\n",
    "        \n",
    "        return support_imgs, query_imgs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 100  # Number of episodes per epoch\n",
    "\n",
    "def train_with_visualization(model, root_dir, epochs=50, n_way=5, k_shot=5, n_query=15, val_ratio=0.2):\n",
    "    # Get datasets\n",
    "    train_dataset, val_dataset = get_datasets(root_dir, val_ratio)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "    \n",
    "    visualizer = TrainingVisualizer()\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss, train_acc = 0, 0\n",
    "        count = 0\n",
    "        \n",
    "        episode_iterator = EpisodeIterator(train_dataset, n_way, k_shot, n_query)\n",
    "        for support, query in episode_iterator:\n",
    "            support = support.to(device)\n",
    "            query = query.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(support, query)\n",
    "            labels = torch.arange(n_way).repeat(n_query).to(device)\n",
    "            \n",
    "            # Compute loss and accuracy\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_acc += acc.item()\n",
    "            count += 1\n",
    "        \n",
    "        train_loss /= count\n",
    "        train_acc = 100 * train_acc / count\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss, val_acc = 0, 0\n",
    "        count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            episode_iterator = EpisodeIterator(val_dataset, n_way, k_shot, n_query)\n",
    "            for support, query in episode_iterator:\n",
    "                support = support.to(device)\n",
    "                query = query.to(device)\n",
    "                \n",
    "                logits = model(support, query)\n",
    "                labels = torch.arange(n_way).repeat(n_query).to(device)\n",
    "                \n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                acc = (logits.argmax(dim=1) == labels).float().mean()\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_acc += acc.item()\n",
    "                count += 1\n",
    "        \n",
    "        val_loss /= count\n",
    "        val_acc = 100 * val_acc / count\n",
    "        \n",
    "        # Update visualizer\n",
    "        visualizer.update(train_loss, val_loss, train_acc, val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}: '\n",
    "              f'Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | '\n",
    "              f'Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        scheduler.step()\n",
    "    \n",
    "    visualizer.save('results')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35244950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your model\n",
    "model = PrototypicalNetwork(encoder=encoder).to(device)\n",
    "\n",
    "# Train with visualization\n",
    "trained_model = train_with_visualization(\n",
    "    model,\n",
    "    root_dir=\"./dataset\",\n",
    "    epochs=100,\n",
    "    n_way=5,\n",
    "    k_shot=5,\n",
    "    n_query=15,\n",
    "    val_ratio=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294578b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
